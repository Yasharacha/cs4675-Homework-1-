# CS4567 HW1 — Web Crawling and Crawl Analysis

## Overview
This project demonstrates hands-on experience with a web crawler and post-processing of crawled data.  
We used **Apache Nutch** to crawl the web and **custom post-processing scripts** to extract keywords and analyze crawl statistics.

The crawler was configured as a **focused crawler** targeting the Georgia Tech College of Computing website:


The crawl successfully fetched **1,089 pages**, exceeding the requirement of at least 1,000 pages.

---

## Tools and Technologies Used

### Web Crawling
- **Apache Nutch 1.21**
- Java **OpenJDK 11**
- WSL2 (Ubuntu) on Windows 11

Nutch was responsible for:
- URL discovery and frontier management
- Politeness and robots.txt handling
- Fetching web pages
- Parsing HTML into plain text
- Storing crawl metadata and content in segment-based archives

### Post-Processing and Analysis
- **Python 3**
- Custom scripts for:
  - Keyword extraction
  - Token frequency analysis
  - Crawl statistics summarization

Apache Lucene was not used directly for crawling; instead, Nutch handled crawling and parsing, while post-processing was performed offline on the extracted text.

---

## Repository Structure
HW1/

├── nutch-seeds/

│ └── seed.txt

├── segment_dump/

│ └── <dumped readable segments>

├── keywords.py

├── keyword_stats.csv

├── README.md


### Excluded from Version Control
The following directory is **not included** in the repository:

nutch-crawl/

This directory contains raw crawl data (segments, crawldb, content, parse_text), which is:
- Very large in size
- Not suitable for GitHub due to storage limits

Instead, **representative dumps** (`segment_dump/`) and **crawl statistics** are included to document results.



## How to Run the Web Crawler

### Environment Setup
The crawler is run in **WSL (Ubuntu)**.

Install Java 11:
sudo apt update
sudo apt install -y openjdk-11-jdk

SET JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export PATH=$JAVA_HOME/bin:$PATH

Configure Crawler:
conf/nutch-site.xml: sets crawler identity, politeness, and fetch behavior
conf/regex-urlfilter.txt: restricts crawling to cc.gatech.edu
seed.txt: contains the initial seed URL

Run the Crawl:
bin/crawl -s /mnt/c/Users/yasha/CS4567/HW1/nutch-seeds \
  -D generate.max.count=1000 \
  /mnt/c/Users/yasha/CS4567/HW1/nutch-crawl \
  7
The crawler can be safely stopped with Ctrl+C after reaching the desired number of fetched pages.

### View Crawl Statistics
bin/nutch readdb /mnt/c/Users/yasha/CS4567/HW1/nutch-crawl/crawldb -stats

Post-Processing and Keyword Extraction
What Was Processed:
- Parsed page text generated by Nutch (parse_text)
- Text extracted from crawled HTML pages
- Pages successfully fetched (db_fetched)
- Keyword Extraction Method
- Text was tokenized using regular expressions
- All tokens were lowercased
- Common stopwords (e.g., “the”, “and”, “of”) were removed
- Remaining words were counted to identify high-frequency keyword

Running Keyword extraction:
python keywords.py

This produces:
Console output of top keywords
keyword_stats.csv for Excel plots

Crawl Statistics Summary
- Total URLs discovered: 11,080
- Pages successfully fetched: 1,089
- Crawl duration: ~154 minutes
- Average crawl speed: ~7.1 pages/min
- Crawl coverage ratio: ~9.8%
